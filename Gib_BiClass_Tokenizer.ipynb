{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Gib_BiClass_Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87966f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43661fff-3986-423c-eb46-74e50aca5057"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# mount your google drive to load/save data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "d87966f3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "636027e5"
      },
      "source": [
        "### Function"
      ],
      "id": "636027e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c4aa28a"
      },
      "source": [
        "############ Get Data ##########\n",
        "# TODO: review encoding\n",
        "# NOTE: f.read() reads as chars, f.readlines() makes a list separated by newlines\n",
        "def get_raw_data(file_path):\n",
        "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "        raw_lines = f.readlines()\n",
        "    return raw_lines"
      ],
      "id": "2c4aa28a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39d75078"
      },
      "source": [
        "def create_index(texts, filename):\n",
        "    words = texts.split()\n",
        "    tokenizer = Tokenizer(num_words=1000000)\n",
        "    tokenizer.fit_on_texts(words)\n",
        "    sequences = tokenizer.texts_to_sequences(words)\n",
        "    word_index = tokenizer.word_index\n",
        "    print(f\"Found {len(word_index)} unique words.\")\n",
        "    with open (filename, \"w\") as f:\n",
        "        json.dump(word_index,f, indent=4)"
      ],
      "id": "39d75078",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1d0122"
      },
      "source": [
        "def get_index(filename):\n",
        "    with open(filename,\"r\") as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "id": "af1d0122",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ca1b8e8"
      },
      "source": [
        "def create_sents(text):\n",
        "    sentences = text.split(\"\\n\")\n",
        "    return sentences"
      ],
      "id": "0ca1b8e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6625637"
      },
      "source": [
        "def padding_data(sentences, index, maxlen=280): # convert string sentences to numerical array for model to understand\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = text_to_word_sequence(sentence)\n",
        "        new_sentence = []\n",
        "        words = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                word = index[word]\n",
        "            except:\n",
        "                KeyError\n",
        "                word = 0\n",
        "                # When encountering new text, the model needs to understand it numerically\n",
        "                # Making it a 0 is called \"zero padding\"\n",
        "            words.append(word)\n",
        "        new_sentence.append(words)\n",
        "        new_sentence = preprocessing.sequence.pad_sequences(new_sentence, maxlen=maxlen, padding=\"post\") #\"post\" has padding at the end, whereas standard does it in the front\n",
        "        new_sentences.append(new_sentence[0])\n",
        "    return new_sentences"
      ],
      "id": "e6625637",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07bb7ab9"
      },
      "source": [
        "def reverse_index(word_index):\n",
        "    reverse_word_index = {value:key for (key,value) in word_index.items()} # Straight from Keras docs\n",
        "    return reverse_word_index\n",
        "\n",
        "def reconst_text(text, reverse_word_index):\n",
        "    # reconstitutes text as a series of words\n",
        "    return (\" \".join([reverse_word_index.get(i, \"?\") for i in text]))\n",
        "    # Because 0 is not in the word index, it needs to be replaced with something. Here, \"?\""
      ],
      "id": "07bb7ab9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9a982cf"
      },
      "source": [
        "################# Labelling the Data ###############\n",
        "\n",
        "def label_data(sentences, label):\n",
        "    total_chunks = []\n",
        "    for sentence in sentences:\n",
        "        total_chunks.append((sentence, label))\n",
        "    return total_chunks"
      ],
      "id": "d9a982cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ff50d8"
      },
      "source": [
        "########## Create Training Data ###########\n",
        "def create_training(total_chunks, cutoff):\n",
        "    random.shuffle(total_chunks)\n",
        "    training_data = []\n",
        "    training_labels = []\n",
        "    testing_data = []\n",
        "    testing_labels = []\n",
        "    # cutoff is from 0 to 1 and determines ratio of chunk to be testing or training data\n",
        "    test_num = len(total_chunks)*cutoff\n",
        "    x = 0\n",
        "    for entry in total_chunks:\n",
        "        if x > test_num:\n",
        "            testing_data.append(entry[0])\n",
        "            testing_labels.append(entry[1])\n",
        "        else:\n",
        "            training_data.append(entry[0])\n",
        "            training_labels.append(entry[1])\n",
        "        x += 1\n",
        "    training_data = np.array(training_data)\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_data = np.array(testing_data)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "    \n",
        "    return training_data, training_labels, testing_data, testing_labels"
      ],
      "id": "73ff50d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6e6960a"
      },
      "source": [
        "################# Creating Model ####################\n",
        "def create_model():\n",
        "    model = keras.Sequential() # Has exactly one input tensor and one output tensor\n",
        "\n",
        "    model.add(keras.layers.Embedding(1000000, 25)) # When the NN looks at the incoming data and starts vectorizing and performing word embedding\n",
        "        # First arg is breadth of data, second is dimenion of words, the shape of the embedding layer\n",
        "    model.add(keras.layers.GlobalAveragePooling1D()) # Allows for NN to flatten data and understand more quickly\n",
        "    model.add(keras.layers.Dense(16, activation=\"relu\")) # Most commonly used layer for text. First arg is dimensions\n",
        "        # Activation is the mathematical process performed on the data\n",
        "    model.add(keras.layers.Dense(16, activation=\"tanh\"))\n",
        "    model.add(keras.layers.Dense(1, activation=\"sigmoid\")) #output\n",
        "        # Sigmoid is common binary output for text classifications, because it interprets data in a binary way\n",
        "    model.summary()\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "id": "f6e6960a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae3972ed"
      },
      "source": [
        "################# Train Model #######################\n",
        "def train_model(model, tt_data, val_size=.3, epochs=1, batch_size=16):\n",
        "    vals = int(len(tt_data[0])*val_size)\n",
        "    training_data = tt_data[0]\n",
        "    training_labels = tt_data[1]\n",
        "    testing_data = tt_data[2]\n",
        "    testing_labels = tt_data[3]\n",
        "    x_val = training_data[:vals]\n",
        "    x_train = training_data[vals:]\n",
        "\n",
        "    y_val = training_labels[:vals]\n",
        "    y_train = training_labels[vals:]\n",
        "\n",
        "    fitModel = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val,y_val), verbose=1, shuffle=True)\n",
        "    model_results = model.evaluate(testing_data, testing_labels)\n",
        "\n",
        "    return model"
      ],
      "id": "ae3972ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43584526"
      },
      "source": [
        "### Main"
      ],
      "id": "43584526"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZWs4riFaqrG"
      },
      "source": [
        "To improve model: \n",
        "\n",
        "*   Set tm_30_2 epochs to 20 to prevent overfitting\n",
        "*   Compare the data rejected by tm30 and tm302"
      ],
      "id": "iZWs4riFaqrG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6f66df65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27bc536-f5da-4be2-9505-1c06a00d8bae"
      },
      "source": [
        "gib_data_path = \"/content/gdrive/MyDrive/Colab Notebooks/training/gib_master\"\n",
        "gold_data_path = \"/content/gdrive/MyDrive/Colab Notebooks/training/gold_master\"\n",
        "\n",
        "gib_data = get_raw_data(gib_data_path)\n",
        "gold_data = get_raw_data(gold_data_path)\n",
        "\n",
        "#create_index(gold_data, \"/content/gdrive/MyDrive/Colab Notebooks/word_index\")\n",
        "\n",
        "word_index = get_index(\"/content/gdrive/MyDrive/Colab Notebooks/word_index\")\n",
        "reverse_word_index = reverse_index(word_index)\n",
        "\n",
        "#gold_words = gold_data.split()\n",
        "#gib_words = gib_data.split()\n",
        "\n",
        "gold_sents = create_sents(gold_data)\n",
        "gib_sents = create_sents(gib_data)\n",
        "\n",
        "#print(gold_sents[:10])\n",
        "#print(gib_sents[:10])\n",
        "\n",
        "gold_padded = padding_data(gold_sents, word_index, maxlen=280)\n",
        "gib_padded = padding_data(gib_sents, word_index, maxlen=280)\n",
        "\n",
        "print(gold_padded[0])\n",
        "print(gib_padded[0])\n",
        "\n",
        "gold_labeled = label_data(gold_padded, 0)\n",
        "gib_labeled = label_data(gib_padded, 1)\n",
        "\n",
        "print(gold_labeled[0])\n",
        "print(gib_labeled[0])\n",
        "\n",
        "all_data = gold_labeled + gib_labeled\n",
        "tt_data = create_training(all_data, cutoff=.9)\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model = train_model(model, tt_data=tt_data, epochs=20, batch_size=16)\n",
        "\n",
        "model.save(\"/content/gdrive/MyDrive/Colab Notebooks/models/token_model_20\")"
      ],
      "id": "6f66df65",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1087  7558     1   246   338 10717     1   628     2     1   686    19\n",
            " 10718   230     6  1088     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "(array([ 1087,  7558,     1,   246,   338, 10717,     1,   628,     2,\n",
            "           1,   686,    19, 10718,   230,     6,  1088,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0], dtype=int32), 0)\n",
            "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 25)          25000000  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                416       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 25,000,705\n",
            "Trainable params: 25,000,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "855/855 [==============================] - 230s 267ms/step - loss: 0.4316 - accuracy: 0.8085 - val_loss: 0.2006 - val_accuracy: 0.9360\n",
            "Epoch 2/20\n",
            "855/855 [==============================] - 227s 265ms/step - loss: 0.1796 - accuracy: 0.9373 - val_loss: 0.1586 - val_accuracy: 0.9546\n",
            "Epoch 3/20\n",
            "855/855 [==============================] - 227s 266ms/step - loss: 0.1505 - accuracy: 0.9494 - val_loss: 0.1530 - val_accuracy: 0.9642\n",
            "Epoch 4/20\n",
            "855/855 [==============================] - 226s 264ms/step - loss: 0.1276 - accuracy: 0.9586 - val_loss: 0.1429 - val_accuracy: 0.9461\n",
            "Epoch 5/20\n",
            "855/855 [==============================] - 227s 266ms/step - loss: 0.1100 - accuracy: 0.9647 - val_loss: 0.1149 - val_accuracy: 0.9666\n",
            "Epoch 6/20\n",
            "855/855 [==============================] - 226s 264ms/step - loss: 0.0936 - accuracy: 0.9709 - val_loss: 0.1046 - val_accuracy: 0.9683\n",
            "Epoch 7/20\n",
            "855/855 [==============================] - 227s 265ms/step - loss: 0.0819 - accuracy: 0.9748 - val_loss: 0.1521 - val_accuracy: 0.9505\n",
            "Epoch 8/20\n",
            "855/855 [==============================] - 227s 265ms/step - loss: 0.0716 - accuracy: 0.9777 - val_loss: 0.1002 - val_accuracy: 0.9645\n",
            "Epoch 9/20\n",
            "855/855 [==============================] - 226s 264ms/step - loss: 0.0586 - accuracy: 0.9813 - val_loss: 0.1024 - val_accuracy: 0.9648\n",
            "Epoch 10/20\n",
            "855/855 [==============================] - 228s 266ms/step - loss: 0.0530 - accuracy: 0.9840 - val_loss: 0.0920 - val_accuracy: 0.9718\n",
            "Epoch 11/20\n",
            "855/855 [==============================] - 229s 268ms/step - loss: 0.0469 - accuracy: 0.9851 - val_loss: 0.1325 - val_accuracy: 0.9609\n",
            "Epoch 12/20\n",
            "855/855 [==============================] - 229s 268ms/step - loss: 0.0401 - accuracy: 0.9873 - val_loss: 0.1558 - val_accuracy: 0.9584\n",
            "Epoch 13/20\n",
            "855/855 [==============================] - 228s 267ms/step - loss: 0.0368 - accuracy: 0.9898 - val_loss: 0.1747 - val_accuracy: 0.9578\n",
            "Epoch 14/20\n",
            "855/855 [==============================] - 229s 268ms/step - loss: 0.0310 - accuracy: 0.9907 - val_loss: 0.2140 - val_accuracy: 0.9529\n",
            "Epoch 15/20\n",
            "855/855 [==============================] - 230s 269ms/step - loss: 0.0310 - accuracy: 0.9914 - val_loss: 0.2409 - val_accuracy: 0.9485\n",
            "Epoch 16/20\n",
            "855/855 [==============================] - 230s 268ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.1182 - val_accuracy: 0.9735\n",
            "Epoch 17/20\n",
            "855/855 [==============================] - 230s 269ms/step - loss: 0.0222 - accuracy: 0.9941 - val_loss: 0.1052 - val_accuracy: 0.9706\n",
            "Epoch 18/20\n",
            "855/855 [==============================] - 229s 268ms/step - loss: 0.0291 - accuracy: 0.9924 - val_loss: 0.1183 - val_accuracy: 0.9684\n",
            "Epoch 19/20\n",
            "855/855 [==============================] - 228s 267ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.1858 - val_accuracy: 0.9606\n",
            "Epoch 20/20\n",
            "855/855 [==============================] - 231s 271ms/step - loss: 0.0241 - accuracy: 0.9922 - val_loss: 0.1120 - val_accuracy: 0.9713\n",
            "68/68 [==============================] - 0s 3ms/step - loss: 0.1146 - accuracy: 0.9774\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/models/token_model_20/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20cc1ef1"
      },
      "source": [
        ""
      ],
      "id": "20cc1ef1",
      "execution_count": null,
      "outputs": []
    }
  ]
}