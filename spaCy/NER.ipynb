{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import json\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "    \n",
    "def save_data(file, data):\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO CREATE PATTERNS FROM GAZETTEER BEFORE TRYING TO RECOGNIZE IN TEXT\n",
    "def create_training_data(file, type):\n",
    "    data = file\n",
    "    patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\n",
    "                    \"label\": type,\n",
    "                    \"pattern\": item\n",
    "                  }\n",
    "        patterns.append(pattern)\n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = create_training_data(load_data(\"data/camps.json\"), \"CONC_CAMP\")\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(patterns):\n",
    "    nlp = English()\n",
    "    ruler = EntityRuler(nlp)\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.add_pipe(ruler)\n",
    "    nlp.to_disk(\"cc_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_rules(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_pattern = re.compile(r'^(PREFACE).*(interview.)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finds all camps in a text thanks to adding patterns to a spacy model that was made to use patterns generated above, which were made from a gazeteer\n",
    "nlp = spacy.load(\"cc_ner\")\n",
    "\n",
    "with open(\"data/interview_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_text = f.read()\n",
    "    questions = test_text.split(\"Q:\")\n",
    "    for question in questions:\n",
    "        question = question.strip().replace(\"\\n\", \" \")\n",
    "        #question = question.replace(\"\\n\", \" \")\n",
    "        # camps from the created patterns are the only entities it can recognize\n",
    "        doc = nlp(question)\n",
    "        for ent in doc.ents:\n",
    "            print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_ents(model, text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities)>0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training data\n",
    "nlp = spacy.load(\"cc_ner\")\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "with open(\"data/interview_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_text = f.read()\n",
    "    questions = test_text.split(\"Q:\")\n",
    "    \n",
    "    for question in questions:\n",
    "        question = question.strip()\n",
    "        question = question.replace(\"\\n\", \" \")\n",
    "        results = seg_ents(nlp, question)\n",
    "        if results != None:\n",
    "            TRAIN_DATA.append(results)\n",
    "\n",
    "save_data(\"data/cc_training_data.json\", TRAIN_DATA)\n",
    "    \n",
    "#     results = []\n",
    "#     entities = []\n",
    "    \n",
    "#     for question in questions:\n",
    "#         question = question.strip().replace(\"\\n\", \" \")\n",
    "#         doc = nlp(question)\n",
    "#         for ent in doc.ents:\n",
    "#             entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "#             #something = ((ent.start_char, ent.end_char, ent.label_))\n",
    "#             #print(something)\n",
    "#         if len(entities)>0:\n",
    "#             results = [question, {\"entities\": entities}]\n",
    "#             print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, text):\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    for ent in doc.ents:\n",
    "        results.append(ent.text)\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"cc_ner\")\n",
    "\n",
    "ie_data = {}\n",
    "\n",
    "with open(\"data/interview_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    hits = []\n",
    "    \n",
    "    questions = text.split(\"Q:\")[:10]\n",
    "    for question in questions:\n",
    "        print(question)\n",
    "    \n",
    "#     paragraphs = text.split(\"\\n\\n\")[:2]\n",
    "#     for paragraph in paragraphs:\n",
    "#         paragraph = paragraph.strip()\n",
    "#         paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "#         results = test_model(nlp, paragraph)\n",
    "#         for result in results:\n",
    "#             hits.append(result)\n",
    "#         ie_data[chapter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "ner.add_label(\"CONC_CAMP\")\n",
    "\n",
    "nlp.add_pipe(ner, name=\"conc_camp_ner\")\n",
    "\n",
    "nlp.to_disk(\"holocaust_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training data for the model\n",
    "def create_patterns(text):\n",
    "    nlp = English()\n",
    "    doc = nlp(text)\n",
    "    results = []\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "        #print(results)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(file, type):\n",
    "    data = file\n",
    "    patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\n",
    "                    \"label\": type,\n",
    "                    \"pattern\": item\n",
    "                  }\n",
    "        patterns.append(pattern)\n",
    "    return(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"cc_ner\")\n",
    "# TRAIN_DATA = []\n",
    "# # Opening the raw text to be analyzed\n",
    "# with open (\"data/text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     segments = f.read().split(\"\\n\\n\")#[3:4] # \\n\\n for double new line. Meaning, if there's a whole newline between two sentences\n",
    "    \n",
    "#     for segment in segments:\n",
    "#         segment = segment.strip()\n",
    "#         segment = segment.replace(\"\\n\", \" \")\n",
    "#         #print(segment)\n",
    "#         results = test_model(nlp, segment)\n",
    "#         if results != None:\n",
    "#             TRAIN_DATA.append(results)\n",
    "#     #print(text)\n",
    "\n",
    "# print(TRAIN_DATA[0])\n",
    "\n",
    "# save_data(\"data/cc_training_data.json\", TRAIN_DATA)\n",
    "\n",
    "with open(\"interviewtext.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    segments = f.read().split(\"\\n\\n\")\n",
    "    for segment in segments:\n",
    "        patterns = create_training_data(segment, \"CONC_CAMP\")\n",
    "\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "nlp = English()\n",
    "\n",
    "with open(\"interviewtext.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    segments = f.read().split(\"\\n\\n\")\n",
    "    \n",
    "    for segment in segments:\n",
    "        segments = segment.strip()\n",
    "        segment = segment.replace(\"\\n\", \" \")\n",
    "        #print(segment)\n",
    "        results = create_patterns(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "\n",
    "nlp.add_pipe(ner, name=\"conc_camp_ner\")\n",
    "ner.add_label(\"CONC_CAMP\")\n",
    "\n",
    "\n",
    "nlp.to_disk(\"holocaust_ner\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
